{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Let's import the needed libraries for our notebook","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport glob\nimport matplotlib.pyplot as plt\nimport numpy as np # linear algebra\nimport os\nimport pathlib\nimport PIL\nimport PIL.Image\nimport seaborn as sns\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n\nfrom IPython.display import clear_output\nfrom keras.utils.vis_utils import plot_model\nfrom sklearn.metrics import confusion_matrix\nfrom tensorflow.keras import callbacks\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2023-07-12T01:34:01.899178Z","iopub.execute_input":"2023-07-12T01:34:01.899609Z","iopub.status.idle":"2023-07-12T01:34:16.322116Z","shell.execute_reply.started":"2023-07-12T01:34:01.899576Z","shell.execute_reply":"2023-07-12T01:34:16.320807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![](https://i.ibb.co/7YqhnXg/wallpaper.png)","metadata":{}},{"cell_type":"markdown","source":"Yes I took this photo using my lightbox and Sony a6400 + Sigma 18-50 f2.8","metadata":{}},{"cell_type":"markdown","source":"## Let's declare some variables","metadata":{}},{"cell_type":"code","source":"batch_size = 8\nimg_height = 1080\nimg_width = 1620\nresized_height = 384\nresized_width = 384\nepochs=150","metadata":{"execution":{"iopub.status.busy":"2023-07-12T01:34:16.324355Z","iopub.execute_input":"2023-07-12T01:34:16.325212Z","iopub.status.idle":"2023-07-12T01:34:16.332179Z","shell.execute_reply.started":"2023-07-12T01:34:16.325174Z","shell.execute_reply":"2023-07-12T01:34:16.329875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's get all the images and let's get a count","metadata":{}},{"cell_type":"code","source":"training_dir = '/kaggle/input/nespresso-capsules-dataset/'\ntraining_dir = pathlib.Path(training_dir)\nprint(len(list(training_dir.glob('*/*.JPG'))))","metadata":{"execution":{"iopub.status.busy":"2023-07-12T01:34:16.333954Z","iopub.execute_input":"2023-07-12T01:34:16.33441Z","iopub.status.idle":"2023-07-12T01:34:16.610887Z","shell.execute_reply.started":"2023-07-12T01:34:16.334373Z","shell.execute_reply":"2023-07-12T01:34:16.609411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's create the training dataset which will be 80% of the total # of images","metadata":{"execution":{"iopub.execute_input":"2023-07-06T15:42:26.118258Z","iopub.status.busy":"2023-07-06T15:42:26.117828Z","iopub.status.idle":"2023-07-06T15:42:26.125992Z","shell.execute_reply":"2023-07-06T15:42:26.12441Z","shell.execute_reply.started":"2023-07-06T15:42:26.118229Z"}}},{"cell_type":"code","source":"train_ds = tf.keras.utils.image_dataset_from_directory(\n  training_dir,\n  validation_split=0.2,\n  subset=\"training\",\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2023-07-12T01:34:16.613552Z","iopub.execute_input":"2023-07-12T01:34:16.613984Z","iopub.status.idle":"2023-07-12T01:34:16.871521Z","shell.execute_reply.started":"2023-07-12T01:34:16.613921Z","shell.execute_reply":"2023-07-12T01:34:16.870298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## As we are using the .repeat() keyword below in the train_ds, we need to get the steps per epoch or loop infinitely","metadata":{}},{"cell_type":"code","source":"steps_per_epoch = len(train_ds)","metadata":{"execution":{"iopub.status.busy":"2023-07-12T01:34:16.872795Z","iopub.execute_input":"2023-07-12T01:34:16.873146Z","iopub.status.idle":"2023-07-12T01:34:16.880394Z","shell.execute_reply.started":"2023-07-12T01:34:16.873116Z","shell.execute_reply":"2023-07-12T01:34:16.878996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Then let's use the remaining 20% as the validation/ test dataset","metadata":{}},{"cell_type":"code","source":"val_ds = tf.keras.utils.image_dataset_from_directory(\n  training_dir,\n  validation_split=0.2,\n  subset=\"validation\",\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2023-07-12T01:34:16.88233Z","iopub.execute_input":"2023-07-12T01:34:16.882765Z","iopub.status.idle":"2023-07-12T01:34:16.982781Z","shell.execute_reply.started":"2023-07-12T01:34:16.882731Z","shell.execute_reply":"2023-07-12T01:34:16.981537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's look at the classes","metadata":{}},{"cell_type":"code","source":"class_names = train_ds.class_names\nprint(class_names)","metadata":{"execution":{"iopub.status.busy":"2023-07-12T01:34:16.984536Z","iopub.execute_input":"2023-07-12T01:34:16.984913Z","iopub.status.idle":"2023-07-12T01:34:16.990614Z","shell.execute_reply.started":"2023-07-12T01:34:16.98488Z","shell.execute_reply":"2023-07-12T01:34:16.989479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's show a random actual image per class","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16, 16))\n    \nfor i in range(len(class_names)):\n    filtered_ds = train_ds.filter(lambda x, l: tf.math.equal(l[0], i))\n    for image, label in filtered_ds.take(1):\n        ax = plt.subplot(4, 4, i+1)\n        plt.imshow(image[0].numpy().astype('uint8'))\n        plt.title(class_names[label.numpy()[0]])\n        plt.axis('off')    ","metadata":{"execution":{"iopub.status.busy":"2023-07-12T01:34:16.992452Z","iopub.execute_input":"2023-07-12T01:34:16.9939Z","iopub.status.idle":"2023-07-12T01:34:53.658488Z","shell.execute_reply.started":"2023-07-12T01:34:16.993863Z","shell.execute_reply":"2023-07-12T01:34:53.65756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's use AUTOTUNE to help dynamically configure and tweak optimal resource allocation during runtime https://stackoverflow.com/questions/56613155/tensorflow-tf-data-autotune","metadata":{}},{"cell_type":"code","source":"AUTOTUNE = tf.data.AUTOTUNE\ntrain_ds = train_ds.cache().shuffle(1000).repeat().prefetch(buffer_size=AUTOTUNE)\nval_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2023-07-12T01:34:53.660098Z","iopub.execute_input":"2023-07-12T01:34:53.660785Z","iopub.status.idle":"2023-07-12T01:34:53.680354Z","shell.execute_reply.started":"2023-07-12T01:34:53.660752Z","shell.execute_reply":"2023-07-12T01:34:53.679094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's create the augmentation layer that we will plug into our model's sequential layer","metadata":{}},{"cell_type":"code","source":"data_augmentation_layer = tf.keras.Sequential([      \n    tf.keras.layers.RandomRotation(0.15),\n    tf.keras.layers.RandomZoom(0.15),\n    tf.keras.layers.RandomTranslation(height_factor=(-0.05, 0.05), width_factor=(-0.05, 0.05)),\n    tf.keras.layers.RandomFlip('horizontal_and_vertical',input_shape=(img_width, img_height, 3)),\n    tf.keras.layers.CenterCrop(height=img_height,width=img_height), #knowing that the image is 1080 at the shortest, we crop from the center to get a 1080 x 1080 image\n    tf.keras.layers.Resizing(height=resized_height,width=resized_width), #after cropping 1080 x 1080 from the center, we now resize down to our intended image that will be passed to the model\n], name='data_augmentation')","metadata":{"execution":{"iopub.status.busy":"2023-07-12T01:34:53.685607Z","iopub.execute_input":"2023-07-12T01:34:53.686048Z","iopub.status.idle":"2023-07-12T01:34:53.715905Z","shell.execute_reply.started":"2023-07-12T01:34:53.686013Z","shell.execute_reply":"2023-07-12T01:34:53.714377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's take one of images and see the augmentations being applied to the image","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 10))\nfor images, _ in train_ds.take(1):\n    for i in range(9):\n        augmented_images = data_augmentation_layer(images)\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(augmented_images[1].numpy().astype(\"uint8\"))\n        plt.axis(\"off\")","metadata":{"execution":{"iopub.status.busy":"2023-07-12T01:34:53.717635Z","iopub.execute_input":"2023-07-12T01:34:53.718678Z","iopub.status.idle":"2023-07-12T01:35:27.731952Z","shell.execute_reply.started":"2023-07-12T01:34:53.71864Z","shell.execute_reply":"2023-07-12T01:35:27.730755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's define our sequential model","metadata":{}},{"cell_type":"code","source":"num_classes = len(class_names)\n\nmodel = Sequential([\n    layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n    data_augmentation_layer, \n    layers.Conv2D(16, 3, padding='same', activation='relu'),\n    layers.MaxPooling2D(),\n    layers.Conv2D(32, 3, padding='same', activation='relu'),\n    layers.MaxPooling2D(),\n    layers.Conv2D(64, 3, padding='same', activation='relu'),\n    layers.MaxPooling2D(),    \n    layers.GlobalAveragePooling2D(),\n    layers.Flatten(),    \n    layers.Dropout(0.2),\n    layers.Dense(512, activation='relu'),  \n    layers.BatchNormalization(),\n    layers.Dense(num_classes, activation='softmax'),  \n])","metadata":{"execution":{"iopub.status.busy":"2023-07-12T01:35:27.733407Z","iopub.execute_input":"2023-07-12T01:35:27.733828Z","iopub.status.idle":"2023-07-12T01:35:27.768555Z","shell.execute_reply.started":"2023-07-12T01:35:27.733793Z","shell.execute_reply":"2023-07-12T01:35:27.767261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's compile the model and declare the optimizer, loss, and metrics that the model would use","metadata":{}},{"cell_type":"code","source":"model.compile(\n  optimizer='adam',\n  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n  metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-07-12T01:35:27.770165Z","iopub.execute_input":"2023-07-12T01:35:27.770566Z","iopub.status.idle":"2023-07-12T01:35:27.795964Z","shell.execute_reply.started":"2023-07-12T01:35:27.770535Z","shell.execute_reply":"2023-07-12T01:35:27.794901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's use an early stopping callback to combat overfitting","metadata":{}},{"cell_type":"code","source":"early_stopping = callbacks.EarlyStopping(monitor='loss', mode='auto',min_delta=0.0005, verbose=1, patience=10, restore_best_weights=True)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-07-12T01:35:27.797626Z","iopub.execute_input":"2023-07-12T01:35:27.798354Z","iopub.status.idle":"2023-07-12T01:35:27.804443Z","shell.execute_reply.started":"2023-07-12T01:35:27.798318Z","shell.execute_reply":"2023-07-12T01:35:27.803125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's start training the model","metadata":{}},{"cell_type":"code","source":"history = model.fit(\n    train_ds,\n    epochs=epochs,\n    steps_per_epoch = steps_per_epoch,\n    verbose=1,\n    callbacks = [early_stopping]\n)","metadata":{"execution":{"iopub.status.busy":"2023-07-12T01:35:27.805832Z","iopub.execute_input":"2023-07-12T01:35:27.806204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's look at the model's architecture","metadata":{}},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's get the accuracy and loss of the training process","metadata":{}},{"cell_type":"code","source":"acc = history.history['accuracy']\nloss = history.history['loss']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's graph the accuracy and loss visually check if it's converging","metadata":{}},{"cell_type":"code","source":"epochs_range = range(len(acc))\n\nplt.figure(figsize=(10, 4))\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training Accuracy')\n\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.legend(loc='upper right')\nplt.title('Training Loss')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's plot the model which shows more info compared to the model.summary","metadata":{}},{"cell_type":"code","source":"plot_model(model,show_shapes=True, show_layer_names=True)","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's try to look at some of the images in the validation dataset","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12, 12))\nfor images, labels in val_ds.take(1):\n    for i in range(batch_size):\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title(class_names[labels[i]])\n        plt.axis(\"off\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's evaluate the model using the validation dataset","metadata":{}},{"cell_type":"code","source":"score = model.evaluate(val_ds, verbose=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's print the loss and accuracy","metadata":{}},{"cell_type":"code","source":"print(\"Accuracy: {}%, Loss:{}\".format(score[1]*100, score[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's do the prediction manually using the validation dataset","metadata":{}},{"cell_type":"code","source":"prediction_labels = []\ntrue_labels = []\n\nfor images, labels in val_ds.take(-1):  \n    preds = model.predict(images.numpy(), verbose=0).round()\n    prediction_labels.append(np.argmax(preds, axis = - 1))\n    true_labels.append(labels.numpy())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's get the true and prediction labels and flatten the predictions and true labels which we will visualize with a confusion matrix","metadata":{}},{"cell_type":"code","source":"true_labels = np.concatenate(true_labels).tolist()\nprediction_labels = np.concatenate(prediction_labels).tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's plot the confusion matrix","metadata":{}},{"cell_type":"code","source":"sns.set(rc={'figure.figsize':(8,6)})\n\nax = sns.heatmap(confusion_matrix(true_labels, prediction_labels), annot=True, cmap='Blues', fmt='g')\nax.set_title('Seaborn Confusion Matrix with labels');\nax.set_xlabel('Predicted Values')\nax.set_ylabel('Actual Values ');\n\n## Ticket labels - List must be in alphabetical order\nax.xaxis.set_ticklabels(class_names)\nax.yaxis.set_ticklabels(class_names)\nplt.xticks(rotation=90)\nplt.yticks(rotation=0)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's visualize some of the correct predictions","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16, 16))\nincorrect_images = 0\n\nfor images, labels in val_ds.take(-1):\n    for i in range(len(images)):\n        predicted_label = np.argmax(np.round(model.predict(tf.expand_dims(images[i], 0)))) #this is needed as the model predicts a batch of iamges\n        true_label = labels[i]\n        \n        if((predicted_label ==true_label) & (incorrect_images <=8)):\n            incorrect_images = incorrect_images + 1\n            ax = plt.subplot(3, 3, incorrect_images)         \n            plt.axis(\"off\")\n            plt.imshow(images[i].numpy().astype(\"uint8\"))\n            plt.title(\"True: {} \\nPredicted: {}\".format(class_names[int(true_label)],class_names[int(predicted_label)]))\n            \nclear_output()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's visualize some of the incorrectly predicted portraits","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16, 16))\n\nincorrect_images = 0\n\nfor images, labels in val_ds.take(-1):\n    for i in range(len(images)):\n        predicted_label = np.argmax(np.round(model.predict(tf.expand_dims(images[i], 0)))) #this is needed as the model predicts a batch of iamges\n        true_label = labels[i]\n        \n        if((predicted_label !=true_label) & (incorrect_images <=8)):\n            incorrect_images = incorrect_images + 1\n            ax = plt.subplot(3, 3, incorrect_images)\n            plt.axis(\"off\")\n            plt.imshow(images[i].numpy().astype(\"uint8\"))\n            plt.title(\"True: {} \\nPredicted: {}\".format(class_names[int(true_label)],class_names[int(predicted_label)]))\n            \nclear_output()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Take aways\nWe created a basic image classifying neural network with decent performance, but I'm sure this can be improved with further hyperparameterization and image augmentation techniques, even a newer architecture. \n\nI hope that my notebook and my dataset have taught you a thing or two on how to create a multi-class image classification model with Tensorflow and Keras.\n\nLooking forward to your notebooks and your solutions! Im excited to learn more techniques on Image Classification using Deep Learning!\n\nHappy Kaggling!","metadata":{}}]}